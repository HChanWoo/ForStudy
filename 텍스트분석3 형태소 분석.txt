from konlpy.tag import Okt
from collections import Counter

filepath = 'C:/Users/DFINE13/OneDrive/바탕 화면/한글 더미파일.txt'
f = open(filepath, 'r', encoding='utf-8')
txtDummy = f.read()
f.close()

okt = Okt()

print('\n====================\nnouns : 명사만 추출\n')
# 텍스트에서 명사만 추출
noun = okt.nouns(txtDummy)
countN = Counter(noun)
noun_list = countN.most_common(10)
for v in noun_list :
    print(v)

print('\n====================\nmorphs : 형태소\n')
# 형태소 단위로 나눔 ( 의미를 가지는 최소 단위 )
# 파라미터의 norm은 문장을 정규화, stem은 각 단어에서 어간을 추출한다.
morph = okt.morphs(txtDummy) # 파라미터 사용 방법 : morphs(txtDummy, '사용할 파라미터'=True)
countM = Counter(morph)
morph_list = countM.most_common(10)
for v in morph_list :
    print(v)

print('\n====================\nphrases : 형태소\n')
# 형태소 단위, 텍스트에서 어절을 추출
phrase = okt.phrases(txtDummy)
countP = Counter(phrase)
phrase_list = countP.most_common(10)
for v in phrase_list :
    print(v)
    
print('\n====================\npos : 형태소\n')
'''
형태소 단위, 각 품사를 태깅하는 함수, 주어진 텍스트를 형태소 단위로 나눈 후 나뉘어진 형태소를 해당하는 품사와 
함께 리스트화한다. 파라미터에는 norm, stem, join(나눠진 형태소와 품사를 형태소/품사 형태로 리스트화)가 있다.
'''
pos = okt.pos(txtDummy, norm=True)
countP2 = Counter(pos)
pos_list = countP2.most_common(10)
for v in pos_list :
    print(v)

'''
아래와 같은 방법으로 필요한 부분만 추출할 수 있다
종류 : Noun(명사) / Verb(동사) / Adjective(형용사) / Determiner(관형사) / Adverb(부사) / Conjunction(접속사) / 
       Exclamation(감탄사) / Josa(조사) / PreEomi(선어말어미) / Eomi(어미) / Suffix(접미사) /
       Punctuation(구두점) / Foreign(외국어, 한자 및 기타기호) / Alpha(알파벳) / Number(숫자) /
       Unknown(미등록어) / KoreanParticle(ex. ㅋㅋ) / Hashtag(트위터 해쉬태그 : #히히) / 
       ScreenName(트위터 아이디 : @example) / Email(이메일 주소) / URL(웹주소)
       https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0
'''
# 예시
print('\n====================\npos : 원하는 종류 추출 예\n')
for i in pos_list :
    if i[0][1] == 'Josa' :
        print(i)