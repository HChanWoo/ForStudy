{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 파일에서 ',' 를 기준으로 단어를 분류하여 빈도수 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = 'C:/Users/DFINE13/OneDrive/바탕 화면/test.txt' # 작업할 파일이 저장된 경로\n",
    "fr = open(filename, 'r', encoding='utf-8')\n",
    "txt = fr.read()\n",
    "fr.close()\n",
    "\n",
    "splitTxt = txt.split(',') # ','기준으로 문자열 나누기(공백 미제거)\n",
    "splitTxtExBlank = txt.replace(' ','').split(',') # ','기준으로 문자열 나누기(공백 제거)\n",
    "\n",
    "dictTxt = {}\n",
    "dictTxtExBlank = {}\n",
    "\n",
    "for st in splitTxt :\n",
    "    if st == '' : \n",
    "        continue\n",
    "    if st in dictTxt :\n",
    "        dictTxt[st]+=1\n",
    "    else :\n",
    "        dictTxt[st]=1\n",
    "        \n",
    "for st in splitTxtExBlank :\n",
    "    if st == '' : \n",
    "        continue\n",
    "    if st in dictTxtExBlank :\n",
    "        dictTxtExBlank[st]+=1\n",
    "    else :\n",
    "        dictTxtExBlank[st]=1\n",
    "\n",
    "# ( sorted : 내림차순 정리 ) => ( dict : 튜플->딕셔너리)\n",
    "# 내림차순으로 정리할시 딕셔너리가 튜플로 바뀜\n",
    "dictTxt = dict((a,b) for a,b in sorted(dictTxt.items(), reverse=True, key=lambda item:item[1]))\n",
    "dictTxtExBlank = dict((a,b) for a,b in sorted(dictTxtExBlank.items(), reverse=True, key=lambda item:item[1]))\n",
    "\n",
    "# dictionary 를 dataframe 구조로 바꿈\n",
    "pdTxt = pd.DataFrame(list(dictTxt.items()), columns=['단어','빈도수'])\n",
    "pdTxtExBlank = pd.DataFrame(list(dictTxtExBlank.items()), columns=['단어','빈도수'])\n",
    "\n",
    "# 현재 작업중인 파이썬 파일이 속한 폴더에 해당 파일 이름으로 저장\n",
    "pdTxt.to_csv('CSVTxt.csv',header=False,index=False,encoding='utf-8-sig')\n",
    "pdTxtExBlank.to_csv('CSVTxtExBlank.csv',header=False,index=False,encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어와 빈도수를 통해 word cloud 생성 및 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "cannot open resource",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4c3341bb3e65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# 생성한 dict 기반으로 word cloud 생성 및 출력\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mwc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictTxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mwc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wordcloud_txt.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[1;31m# try to find a position\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0mfont\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruetype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m                 \u001b[1;31m# transpose font optionally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 transposed_font = ImageFont.TransposedFont(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageFont.py\u001b[0m in \u001b[0;36mtruetype\u001b[1;34m(font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfreetype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageFont.py\u001b[0m in \u001b[0;36mfreetype\u001b[1;34m(font)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfreetype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFreeTypeFont\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout_engine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageFont.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, font, size, index, encoding, layout_engine)\u001b[0m\n\u001b[0;32m    191\u001b[0m                         \u001b[0mload_from_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             self.font = core.getfont(\n\u001b[0m\u001b[0;32m    194\u001b[0m                 \u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayout_engine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayout_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             )\n",
      "\u001b[1;31mOSError\u001b[0m: cannot open resource"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "# wordcloud 폰트, 크기 등을 설정\n",
    "wc = WordCloud(font_path='SCDream3.otf',\n",
    "               background_color=\"white\",\n",
    "               width=1000,\n",
    "               height=1000,\n",
    "               max_words=100,\n",
    "               max_font_size=300)\n",
    "\n",
    "# 생성한 dict 기반으로 word cloud 생성 및 출력\n",
    "wc.generate_from_frequencies(dict((a,b) for a,b in sorted(dictTxt.items(), reverse=True, key=lambda item:item[1])))\n",
    "wc.to_file('wordcloud_txt.png')\n",
    "\n",
    "image = img.imread('wordcloud_txt.png')\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "nouns : 명사만 추출\n",
      "\n",
      "('별', 34)\n",
      "('하나', 26)\n",
      "('이름', 24)\n",
      "('까닭', 23)\n",
      "('나', 22)\n",
      "('듯', 13)\n",
      "('어머님', 10)\n",
      "('어머니', 10)\n",
      "('무성', 10)\n",
      "('언덕', 9)\n",
      "\n",
      "====================\n",
      "morphs : 형태소\n",
      "\n",
      "('.', 82)\n",
      "(',', 51)\n",
      "('별', 34)\n",
      "('에', 31)\n",
      "('하나', 26)\n",
      "('이', 24)\n",
      "('는', 24)\n",
      "('이름', 24)\n",
      "('의', 23)\n",
      "('까닭', 23)\n",
      "\n",
      "====================\n",
      "phrases : 형태소\n",
      "\n",
      "('별빛', 1)\n",
      "('어머님', 1)\n",
      "('벌써', 1)\n",
      "('벌써 나', 1)\n",
      "('벌써 나의 이름', 1)\n",
      "('이네들', 1)\n",
      "('마디씩', 1)\n",
      "('마디씩 멀리', 1)\n",
      "('마디씩 멀리 나', 1)\n",
      "('이제', 1)\n",
      "\n",
      "====================\n",
      "pos : 형태소\n",
      "\n",
      "(('.', 'Punctuation'), 82)\n",
      "((',', 'Punctuation'), 51)\n",
      "(('별', 'Noun'), 34)\n",
      "(('에', 'Josa'), 31)\n",
      "(('하나', 'Noun'), 26)\n",
      "(('는', 'Josa'), 24)\n",
      "(('이름', 'Noun'), 24)\n",
      "(('의', 'Josa'), 23)\n",
      "(('까닭', 'Noun'), 23)\n",
      "(('나', 'Noun'), 22)\n",
      "\n",
      "====================\n",
      "pos : 원하는 종류 추출 예\n",
      "\n",
      "(('에', 'Josa'), 31)\n",
      "(('는', 'Josa'), 24)\n",
      "(('의', 'Josa'), 23)\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "filepath = 'C:/Users/DFINE13/OneDrive/바탕 화면/한글 더미파일.txt'\n",
    "f = open(filepath, 'r', encoding='utf-8')\n",
    "txtDummy = f.read()\n",
    "f.close()\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "print('\\n====================\\nnouns : 명사만 추출\\n')\n",
    "# 텍스트에서 명사만 추출\n",
    "noun = okt.nouns(txtDummy)\n",
    "countN = Counter(noun)\n",
    "noun_list = countN.most_common(10)\n",
    "for v in noun_list :\n",
    "    print(v)\n",
    "\n",
    "print('\\n====================\\nmorphs : 형태소\\n')\n",
    "# 형태소 단위로 나눔 ( 의미를 가지는 최소 단위 )\n",
    "# 파라미터의 norm은 문장을 정규화, stem은 각 단어에서 어간을 추출한다.\n",
    "morph = okt.morphs(txtDummy) # 파라미터 사용 방법 : morphs(txtDummy, '사용할 파라미터'=True)\n",
    "countM = Counter(morph)\n",
    "morph_list = countM.most_common(10)\n",
    "for v in morph_list :\n",
    "    print(v)\n",
    "\n",
    "print('\\n====================\\nphrases : 형태소\\n')\n",
    "# 형태소 단위, 텍스트에서 어절을 추출\n",
    "phrase = okt.phrases(txtDummy)\n",
    "countP = Counter(phrase)\n",
    "phrase_list = countP.most_common(10)\n",
    "for v in phrase_list :\n",
    "    print(v)\n",
    "    \n",
    "print('\\n====================\\npos : 형태소\\n')\n",
    "'''\n",
    "형태소 단위, 각 품사를 태깅하는 함수, 주어진 텍스트를 형태소 단위로 나눈 후 나뉘어진 형태소를 해당하는 품사와 \n",
    "함께 리스트화한다. 파라미터에는 norm, stem, join(나눠진 형태소와 품사를 형태소/품사 형태로 리스트화)가 있다.\n",
    "'''\n",
    "pos = okt.pos(txtDummy, norm=True)\n",
    "countP2 = Counter(pos)\n",
    "pos_list = countP2.most_common(10)\n",
    "for v in pos_list :\n",
    "    print(v)\n",
    "\n",
    "'''\n",
    "아래와 같은 방법으로 필요한 부분만 추출할 수 있다\n",
    "종류 : Noun(명사) / Verb(동사) / Adjective(형용사) / Determiner(관형사) / Adverb(부사) / Conjunction(접속사) / \n",
    "       Exclamation(감탄사) / Josa(조사) / PreEomi(선어말어미) / Eomi(어미) / Suffix(접미사) /\n",
    "       Punctuation(구두점) / Foreign(외국어, 한자 및 기타기호) / Alpha(알파벳) / Number(숫자) /\n",
    "       Unknown(미등록어) / KoreanParticle(ex. ㅋㅋ) / Hashtag(트위터 해쉬태그 : #히히) / \n",
    "       ScreenName(트위터 아이디 : @example) / Email(이메일 주소) / URL(웹주소)\n",
    "       https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0\n",
    "'''\n",
    "# 예시\n",
    "print('\\n====================\\npos : 원하는 종류 추출 예\\n')\n",
    "for i in pos_list :\n",
    "    if i[0][1] == 'Josa' :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin konlpy\n",
      "\n",
      "[('내', 'Noun'), ('가', 'Josa'), ('맘대로', 'Noun'), ('안녕', 'Noun'), ('이라고', 'Josa'), ('말', 'Noun'), ('해볼까', 'Verb'), ('?', 'Punctuation')]\n",
      "\n",
      "\n",
      "customized konlpy\n",
      "\n",
      "[('내', 'Noun'), ('가', 'Josa'), ('맘대로', 'Noun'), ('안녕이라고', 'Noun'), ('말해볼까', 'Noun'), ('?', 'Punctuation')]\n",
      "ngram format error : Noun\n",
      "\n",
      "\n",
      "customized konlpy2\n",
      "\n",
      "[('필터', 'Noun'), ('단어', 'Noun'), ('필터', 'Noun'), ('단어', 'Noun'), ('커', 'Verb'), ('스텀', 'Noun'), ('이', 'Josa'), ('될까', 'Verb'), ('요', 'Noun'), ('?', 'Punctuation')]\n",
      "\n",
      "\n",
      "[('필터 - 단어', 'Noun'), ('필터 - 단어', 'Noun'), ('대체 완료', 'Noun'), ('요구르트', 'Noun')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DFINE13\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "from ckonlpy.tag import Postprocessor\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "testStr = '내가 맘대로 안녕이라고 말해볼까?'\n",
    "print('origin konlpy\\n') # 비교를 위한 origin konlpy\n",
    "print(twitter.pos(testStr))\n",
    "\n",
    "twitter.add_dictionary(['안녕이라고','말해볼까'],'Noun') # 해당 리스트 요소들을 Noun으로 재정의?\n",
    "print('\\n\\ncustomized konlpy\\n')\n",
    "print(twitter.pos(testStr))\n",
    "\n",
    "# 해당 단어 필터 등등의 custom konlpy\n",
    "testCustom = '필터단어 필터 단어 커스텀이 될까요?'\n",
    "postprocessor = Postprocessor(\n",
    "    base_tagger = twitter,\n",
    "    stopwords = {'필터'},      # 해당 단어 필터 : {'A'}\n",
    "    #passwords = {'단어'},     # 해당 단어만 선택 {}\n",
    "    passtags = {'Noun'},       # 해당 훔사만 선택 {'Noun'}\n",
    "    replace={'요' : '요구르트', ('스텀','Noun') : '대체 완료'}, # 해당 단어 set 치환 {'A' : 'B', ('C', 'Noun') : 'D'}\n",
    "    ngrams = [('필터','단어'),'Noun'] # 해당 복합 단어 set를 한 단어로 결합 [ (('A', 'B'), 'Noun'), .... ] => A - B,Noun'\n",
    ")\n",
    "\n",
    "print('\\n\\ncustomized konlpy2\\n')\n",
    "print(twitter.pos(testCustom))\n",
    "print('\\n')\n",
    "print(postprocessor.pos(testCustom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('김개발', 'Noun'), 1)\n",
      "(('은', 'Josa'), 1)\n",
      "(('신입사원', 'Noun'), 1)\n",
      "(('이다', 'Josa'), 1)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "konlpy 시스템 사전에 직접 단어 추가하기\n",
    "/konlpy/java/open-korean-text-2.1.0.jar 에서 추가를 원하는 폴더를 선택한다 ( noun을 예로 들겠다 )\n",
    "/org/openkoreantext/processor/util/noun/wikipedia_title_nouns.txt 에서 추가를 원하는 단어를 입력한후 저장하면 된다\n",
    "=> 권한오류 같은 경우 현재 작업중인 환경(jupyter notebook) 등을 닫고 실행하면 된다.\n",
    "=> 해당 경로에서 직접적인 편집이 되지 않아 해당 jar 파일을 압축 해제하여 수정한 후 다시 압축하는 방법으로 해결했다\n",
    "'''\n",
    "\n",
    "testTxtHwang = '김개발은 신입사원이다'\n",
    "pos = okt.pos(testTxtHwang, norm=True)\n",
    "countP2 = Counter(pos)\n",
    "pos_list = countP2.most_common(10)\n",
    "for v in pos_list :\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
